%!TeX root = ../../Tesi.tex
Il metodo di Jacobi (o metodo degli spostamenti simultanei) \`e un metodo iterativo per la risoluzione di sistemi lineari, ideato dal matematico prussiano Carl Gustav Jacob Jacobi (1804-1851)\,\cite{JacobiMethod}.

Se gli elementi sulla diagonale principale di A sono non nulli, possiamo mettere in evidenza in ogni equazione di
\eqref{eq:formaAlgebricaSistemiLineari} la corrispondente incognita, ottenendo il sistema lineare equivalente
\begin{equation}
    x_{i}=\frac{1}{a_{ii}}\Bigg(b_{i} - \sum_{j=1, \, j \neq i}^{n}a_{ij}x_{j}\Bigg),\quad i=1,\dots,n.
\end{equation}

Dato il vettore iniziale $\mathbf{x}^{(0)}$, il metodo di Jacobi calcola $\mathbf{x}^{(k+1)}$ come segue
\begin{equation}
    \label{eq:metodoJacobi}
    x_{i}^{(k+1)}=\frac{1}{a_{ii}}\Bigg(b_{i} - \sum_{j=1, \, j \neq i}^{n}a_{ij}x{_j}^{(k)}\Bigg),\quad i=1,\dots,n.
\end{equation}
La \eqref{eq:metodoJacobi} \`e un caso particolare della decomposizione additiva $A = P-N$ con
\begin{equation*}
    P = D\quad \text{e}\quad N = D - A = E + F,
\end{equation*}
dove $D=diag(a_{ii})\in\mathbb{R}^{n\times n}$ \`e la matrice diagonale contenente gli elementi di $A$ sulla diagonale principale,
$E=(e_{ij})\in\mathbb{R}^{n\times n}$ \`e la matrice triangolare inferiore con \mbox{$e_{ij}=-a_{ij} \ \text{se} \ i>j \ \text{ed}\ e_{ij}=0 \ \text{se} \ i\le j$},
mentre $F=(f_{ij})\in\mathbb{R}^{n\times n}$ \`e la matrice triangolare superiore di coefficienti \mbox{$f_{ij}=-a_{ij} \text{ se } j>i$ e $f_{ij}=0 \text{ se } j\le i$}.\newline
Pertanto $A = D - (E + F)$.\newline
La corrispondente matrice di iterazione $B_{J}$ \`e data da
\begin{equation}
    B_{J} = P^{-1}N = D^{-1}(E + F) = I - D^{-1}A.
\end{equation}
\subsection{Convergenza del metodo di Jacobi}
Esistono particolari classi di matrici per le quali \`e possibile stabilire a priori la convergenza del metodo di Jacobi.

Iniziamo con l'introdurre la definizione di matrice a dominanza diagonale per righe, che si riveler\`a una propriet\`a fondamentale per garantire la convergenza del metodo.
\begin{definizione}
    Sia $M = (m_{ij})\in\mathbb{R}^{n \times n}$ una matrice quadrata di ordine $n\ge 1$, allora $M$ \`e detta a dominanza diagonale per righe se
    \[
        \abs{m_{ii}} \ge \sum_{j=1,\, j \neq i}^{n}\abs{m_{ij}},\quad i = 1,\cdots,n
    \]
    Se le disuguaglianze precedenti sono valide in senso stretto, $M$ \`e detta a dominanza diagonale stretta per righe.
\end{definizione}

Ora possiamo esporre i risultati di convergenza validi per il metodo di Jacobi.
\begin{teorema}
    \label{teo:convergenzaDominanzaDiagonaleStretta}
    Se $A$ \`e una matrice a dominanza diagonale stretta per righe, allora il metodo di Jacobi \`e convergente.
\end{teorema}
\begin{teorema}
    \label{teo:convergenzaSimmetricaDefinitaPositivaJacobi}
    Se $A \ \text{e} \ 2D - A$ sono matrici simmetriche definite positive, allora il metodo di Jacobi converge e $\rho(B_{J}) = \norm{B_{J}}_{A} = \norm{B_{J}}_{D}$.
\end{teorema}
Notiamo come il Teorema \ref{teo:convergenzaSimmetricaDefinitaPositivaJacobi} discenda dal Teorema \ref{teo:convergenzaSimmetricaDefinitaPositiva} con $P = D$.
\subsection{Aspetti computazionali}
Nello scenario peggiore in cui la matrice $A$ sia densa, ovvero la maggior parte dei suoi elementi siano non nulli,
il costo computazionale del metodo di Jacobi \`e dell'ordine di $n^{2}$ \si{\flops} per iterazione: una magnitudine di diversi ordini inferiore
al costo associato alla regola di Cramer.\newline
Al contrario, se $A$ \`e sparsa, cio\`e il numero dei suoi elementi nulli \`e dell'ordine di $n$, allora anche $E \ \text{ed} \ F$
sono sparse, per cui l'esecuzione di un passo dell'algoritmo richiede un numero di operazioni in virgola mobile pari a $n$ e non
pi\`u a $n^2$.

Dobbiamo sottolineare come in \eqref{eq:metodoJacobi} le componenti $x_{i}^{(k+1)}$ del vettore soluzione possano essere calcolate indipendentemente le une dalle altre. \newline
In un sistema a elaborazione parallela, la disponibilit\`a di $n$ processori in grado di eseguire operazioni aritmetiche in simultanea consente di determinare $\mathbf{x}^{(k+1)}$ nell'intervallo di tempo richiesto dal calcolo di una singola componente su un tradizionale sistema monoprocessore.

D'altro canto, il metodo aggiorna le componenti della soluzione approssimata agendo sulle quantit\`a calcolate all'iterazione precedente, caratteristica che non gli consente di raggiungere un'elevata velocit\`a di convergenza visto che il procedimento iterativo sfrutta delle informazioni non aggiornate durante la sua esecuzione.

